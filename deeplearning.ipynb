{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wNUf9waoyBG7",
        "outputId": "fb325fde-c26a-4f64-f785-9c42560083c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "10/10 [==============================] - 2s 38ms/step - loss: 3.6290 - accuracy: 0.0469 - val_loss: 3.6229 - val_accuracy: 0.0370\n",
            "Epoch 2/200\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 3.5919 - accuracy: 0.0938 - val_loss: 3.6027 - val_accuracy: 0.0370\n",
            "Epoch 3/200\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 3.5545 - accuracy: 0.0812 - val_loss: 3.5850 - val_accuracy: 0.0370\n",
            "Epoch 4/200\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 3.5108 - accuracy: 0.0906 - val_loss: 3.5650 - val_accuracy: 0.0741\n",
            "Epoch 5/200\n",
            "10/10 [==============================] - 0s 11ms/step - loss: 3.4563 - accuracy: 0.0969 - val_loss: 3.5366 - val_accuracy: 0.1111\n",
            "Epoch 6/200\n",
            "10/10 [==============================] - 0s 11ms/step - loss: 3.3392 - accuracy: 0.1406 - val_loss: 3.5070 - val_accuracy: 0.1235\n",
            "Epoch 7/200\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 3.2576 - accuracy: 0.1781 - val_loss: 3.4652 - val_accuracy: 0.1481\n",
            "Epoch 8/200\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 3.1281 - accuracy: 0.2281 - val_loss: 3.3808 - val_accuracy: 0.1728\n",
            "Epoch 9/200\n",
            "10/10 [==============================] - 0s 12ms/step - loss: 2.9867 - accuracy: 0.2875 - val_loss: 3.2789 - val_accuracy: 0.1728\n",
            "Epoch 10/200\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 2.7609 - accuracy: 0.2937 - val_loss: 3.1814 - val_accuracy: 0.1481\n",
            "Epoch 11/200\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 2.5908 - accuracy: 0.3594 - val_loss: 3.0519 - val_accuracy: 0.1852\n",
            "Epoch 12/200\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 2.3514 - accuracy: 0.4062 - val_loss: 2.9341 - val_accuracy: 0.2469\n",
            "Epoch 13/200\n",
            "10/10 [==============================] - 0s 13ms/step - loss: 2.2394 - accuracy: 0.4281 - val_loss: 2.8191 - val_accuracy: 0.3086\n",
            "Epoch 14/200\n",
            "10/10 [==============================] - 0s 11ms/step - loss: 2.0404 - accuracy: 0.4812 - val_loss: 2.7038 - val_accuracy: 0.3210\n",
            "Epoch 15/200\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 1.8843 - accuracy: 0.5125 - val_loss: 2.6037 - val_accuracy: 0.3704\n",
            "Epoch 16/200\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 1.7214 - accuracy: 0.5500 - val_loss: 2.4932 - val_accuracy: 0.3704\n",
            "Epoch 17/200\n",
            "10/10 [==============================] - 0s 11ms/step - loss: 1.6068 - accuracy: 0.5875 - val_loss: 2.3985 - val_accuracy: 0.3951\n",
            "Epoch 18/200\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 1.4810 - accuracy: 0.6156 - val_loss: 2.2959 - val_accuracy: 0.4444\n",
            "Epoch 19/200\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 1.4097 - accuracy: 0.6281 - val_loss: 2.2355 - val_accuracy: 0.4691\n",
            "Epoch 20/200\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 1.2845 - accuracy: 0.6781 - val_loss: 2.0967 - val_accuracy: 0.4938\n",
            "Epoch 21/200\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 1.1784 - accuracy: 0.6938 - val_loss: 1.9988 - val_accuracy: 0.4938\n",
            "Epoch 22/200\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 1.0855 - accuracy: 0.7031 - val_loss: 1.9662 - val_accuracy: 0.4938\n",
            "Epoch 23/200\n",
            "10/10 [==============================] - 0s 12ms/step - loss: 1.0079 - accuracy: 0.7437 - val_loss: 1.8253 - val_accuracy: 0.5556\n",
            "Epoch 24/200\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 0.8458 - accuracy: 0.7969 - val_loss: 1.7626 - val_accuracy: 0.5802\n",
            "Epoch 25/200\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 0.7810 - accuracy: 0.8250 - val_loss: 1.6881 - val_accuracy: 0.5802\n",
            "Epoch 26/200\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.7715 - accuracy: 0.7812 - val_loss: 1.5859 - val_accuracy: 0.6543\n",
            "Epoch 27/200\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 0.6833 - accuracy: 0.8250 - val_loss: 1.5319 - val_accuracy: 0.6667\n",
            "Epoch 28/200\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.5840 - accuracy: 0.8406 - val_loss: 1.4632 - val_accuracy: 0.7037\n",
            "Epoch 29/200\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 0.5906 - accuracy: 0.8625 - val_loss: 1.4235 - val_accuracy: 0.6667\n",
            "Epoch 30/200\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.5117 - accuracy: 0.8625 - val_loss: 1.3759 - val_accuracy: 0.6790\n",
            "Epoch 31/200\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 0.4659 - accuracy: 0.9031 - val_loss: 1.3180 - val_accuracy: 0.7160\n",
            "Epoch 32/200\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 0.4635 - accuracy: 0.8938 - val_loss: 1.3087 - val_accuracy: 0.6914\n",
            "Epoch 33/200\n",
            "10/10 [==============================] - 0s 13ms/step - loss: 0.3859 - accuracy: 0.9250 - val_loss: 1.2710 - val_accuracy: 0.7037\n",
            "Epoch 34/200\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.3691 - accuracy: 0.8969 - val_loss: 1.2229 - val_accuracy: 0.7284\n",
            "Epoch 35/200\n",
            "10/10 [==============================] - 0s 11ms/step - loss: 0.3648 - accuracy: 0.9125 - val_loss: 1.2003 - val_accuracy: 0.7037\n",
            "Epoch 36/200\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 0.2980 - accuracy: 0.9438 - val_loss: 1.1891 - val_accuracy: 0.7160\n",
            "Epoch 37/200\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.3289 - accuracy: 0.9250 - val_loss: 1.1186 - val_accuracy: 0.7284\n",
            "Epoch 38/200\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 0.3056 - accuracy: 0.9344 - val_loss: 1.1188 - val_accuracy: 0.7284\n",
            "Epoch 39/200\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.2712 - accuracy: 0.9375 - val_loss: 1.1203 - val_accuracy: 0.7407\n",
            "Epoch 40/200\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 0.2572 - accuracy: 0.9469 - val_loss: 1.1092 - val_accuracy: 0.7407\n",
            "Epoch 41/200\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.2482 - accuracy: 0.9406 - val_loss: 1.0974 - val_accuracy: 0.7531\n",
            "Epoch 42/200\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 0.2241 - accuracy: 0.9625 - val_loss: 1.1073 - val_accuracy: 0.7284\n",
            "Epoch 43/200\n",
            "10/10 [==============================] - 0s 12ms/step - loss: 0.2315 - accuracy: 0.9375 - val_loss: 1.0769 - val_accuracy: 0.7407\n",
            "Epoch 44/200\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.2179 - accuracy: 0.9500 - val_loss: 1.0553 - val_accuracy: 0.7531\n",
            "Epoch 45/200\n",
            "10/10 [==============================] - 0s 11ms/step - loss: 0.2286 - accuracy: 0.9500 - val_loss: 1.0453 - val_accuracy: 0.7531\n",
            "Epoch 46/200\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 0.1817 - accuracy: 0.9750 - val_loss: 1.0529 - val_accuracy: 0.7407\n",
            "Epoch 47/200\n",
            "10/10 [==============================] - 0s 12ms/step - loss: 0.1776 - accuracy: 0.9563 - val_loss: 1.0656 - val_accuracy: 0.7531\n",
            "Epoch 48/200\n",
            "10/10 [==============================] - 0s 12ms/step - loss: 0.2045 - accuracy: 0.9531 - val_loss: 1.0488 - val_accuracy: 0.7407\n",
            "Epoch 49/200\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.1446 - accuracy: 0.9812 - val_loss: 1.0424 - val_accuracy: 0.7407\n",
            "Epoch 50/200\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 0.1489 - accuracy: 0.9719 - val_loss: 1.0414 - val_accuracy: 0.7284\n",
            "Epoch 51/200\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 0.1768 - accuracy: 0.9500 - val_loss: 1.0478 - val_accuracy: 0.7407\n",
            "Epoch 52/200\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.1857 - accuracy: 0.9469 - val_loss: 1.0378 - val_accuracy: 0.7407\n",
            "Epoch 53/200\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 0.1714 - accuracy: 0.9688 - val_loss: 1.0680 - val_accuracy: 0.7407\n",
            "Epoch 54/200\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 0.1404 - accuracy: 0.9688 - val_loss: 1.0850 - val_accuracy: 0.7407\n",
            "Epoch 55/200\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.1122 - accuracy: 0.9750 - val_loss: 1.0687 - val_accuracy: 0.7407\n",
            "Epoch 56/200\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.1334 - accuracy: 0.9750 - val_loss: 1.0441 - val_accuracy: 0.7531\n",
            "Epoch 57/200\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 0.1214 - accuracy: 0.9812 - val_loss: 1.0191 - val_accuracy: 0.7531\n",
            "Epoch 58/200\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.1337 - accuracy: 0.9656 - val_loss: 1.0140 - val_accuracy: 0.7531\n",
            "Epoch 59/200\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.1642 - accuracy: 0.9625 - val_loss: 1.0347 - val_accuracy: 0.7407\n",
            "Epoch 60/200\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 0.1115 - accuracy: 0.9750 - val_loss: 1.0330 - val_accuracy: 0.7531\n",
            "Epoch 61/200\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.1067 - accuracy: 0.9906 - val_loss: 1.0492 - val_accuracy: 0.7531\n",
            "Epoch 62/200\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.1574 - accuracy: 0.9625 - val_loss: 1.0795 - val_accuracy: 0.7531\n",
            "Epoch 63/200\n",
            "10/10 [==============================] - 0s 11ms/step - loss: 0.1171 - accuracy: 0.9688 - val_loss: 1.0901 - val_accuracy: 0.7407\n",
            "Epoch 64/200\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 0.1273 - accuracy: 0.9719 - val_loss: 1.0921 - val_accuracy: 0.7407\n",
            "Epoch 65/200\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.1054 - accuracy: 0.9688 - val_loss: 1.1081 - val_accuracy: 0.7654\n",
            "Epoch 66/200\n",
            "10/10 [==============================] - 0s 11ms/step - loss: 0.0791 - accuracy: 0.9875 - val_loss: 1.1308 - val_accuracy: 0.7531\n",
            "Epoch 67/200\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.0756 - accuracy: 0.9844 - val_loss: 1.1060 - val_accuracy: 0.7531\n",
            "Epoch 68/200\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.0834 - accuracy: 0.9812 - val_loss: 1.1185 - val_accuracy: 0.7531\n",
            "Epoch 69/200\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 0.1086 - accuracy: 0.9750 - val_loss: 1.0987 - val_accuracy: 0.7531\n",
            "Epoch 70/200\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.1036 - accuracy: 0.9656 - val_loss: 1.1097 - val_accuracy: 0.7407\n",
            "Epoch 71/200\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 0.0950 - accuracy: 0.9812 - val_loss: 1.1443 - val_accuracy: 0.7531\n",
            "Epoch 72/200\n",
            "10/10 [==============================] - 0s 12ms/step - loss: 0.0983 - accuracy: 0.9812 - val_loss: 1.1185 - val_accuracy: 0.7531\n",
            "Epoch 73/200\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 0.0805 - accuracy: 0.9812 - val_loss: 1.1089 - val_accuracy: 0.7531\n",
            "Epoch 74/200\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 0.1058 - accuracy: 0.9750 - val_loss: 1.1082 - val_accuracy: 0.7407\n",
            "Epoch 75/200\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 0.0856 - accuracy: 0.9812 - val_loss: 1.1143 - val_accuracy: 0.7407\n",
            "Epoch 76/200\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.0632 - accuracy: 0.9906 - val_loss: 1.1413 - val_accuracy: 0.7284\n",
            "Epoch 77/200\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.0541 - accuracy: 0.9844 - val_loss: 1.1302 - val_accuracy: 0.7407\n",
            "Epoch 78/200\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.0888 - accuracy: 0.9781 - val_loss: 1.1209 - val_accuracy: 0.7284\n",
            "Epoch 79/200\n",
            "10/10 [==============================] - 0s 11ms/step - loss: 0.0549 - accuracy: 0.9937 - val_loss: 1.0986 - val_accuracy: 0.7284\n",
            "Epoch 80/200\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 0.0795 - accuracy: 0.9812 - val_loss: 1.1093 - val_accuracy: 0.7531\n",
            "Epoch 81/200\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 0.0811 - accuracy: 0.9719 - val_loss: 1.1264 - val_accuracy: 0.7531\n",
            "Epoch 82/200\n",
            "10/10 [==============================] - 0s 11ms/step - loss: 0.0614 - accuracy: 0.9844 - val_loss: 1.1037 - val_accuracy: 0.7654\n",
            "Epoch 83/200\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.0654 - accuracy: 0.9875 - val_loss: 1.0723 - val_accuracy: 0.7531\n",
            "Epoch 84/200\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 0.0615 - accuracy: 0.9844 - val_loss: 1.0850 - val_accuracy: 0.7654\n",
            "Epoch 85/200\n",
            "10/10 [==============================] - 0s 14ms/step - loss: 0.0617 - accuracy: 0.9812 - val_loss: 1.0665 - val_accuracy: 0.7654\n",
            "Epoch 86/200\n",
            "10/10 [==============================] - 0s 16ms/step - loss: 0.0711 - accuracy: 0.9750 - val_loss: 1.0909 - val_accuracy: 0.7654\n",
            "Epoch 87/200\n",
            "10/10 [==============================] - 0s 15ms/step - loss: 0.0808 - accuracy: 0.9812 - val_loss: 1.1035 - val_accuracy: 0.7654\n",
            "Epoch 88/200\n",
            "10/10 [==============================] - 0s 15ms/step - loss: 0.0714 - accuracy: 0.9844 - val_loss: 1.1080 - val_accuracy: 0.7654\n",
            "Epoch 89/200\n",
            "10/10 [==============================] - 0s 14ms/step - loss: 0.0688 - accuracy: 0.9781 - val_loss: 1.1347 - val_accuracy: 0.7407\n",
            "Epoch 90/200\n",
            "10/10 [==============================] - 0s 15ms/step - loss: 0.0845 - accuracy: 0.9750 - val_loss: 1.1441 - val_accuracy: 0.7531\n",
            "Epoch 91/200\n",
            "10/10 [==============================] - 0s 15ms/step - loss: 0.0650 - accuracy: 0.9906 - val_loss: 1.1137 - val_accuracy: 0.7531\n",
            "Epoch 92/200\n",
            "10/10 [==============================] - 0s 14ms/step - loss: 0.0564 - accuracy: 0.9844 - val_loss: 1.0863 - val_accuracy: 0.7654\n",
            "Epoch 93/200\n",
            "10/10 [==============================] - 0s 14ms/step - loss: 0.0514 - accuracy: 0.9875 - val_loss: 1.1061 - val_accuracy: 0.7531\n",
            "Epoch 94/200\n",
            "10/10 [==============================] - 0s 13ms/step - loss: 0.0718 - accuracy: 0.9812 - val_loss: 1.1106 - val_accuracy: 0.7654\n",
            "Epoch 95/200\n",
            "10/10 [==============================] - 0s 14ms/step - loss: 0.0731 - accuracy: 0.9781 - val_loss: 1.1140 - val_accuracy: 0.7407\n",
            "Epoch 96/200\n",
            "10/10 [==============================] - 0s 14ms/step - loss: 0.0713 - accuracy: 0.9781 - val_loss: 1.0971 - val_accuracy: 0.7284\n",
            "Epoch 97/200\n",
            "10/10 [==============================] - 0s 15ms/step - loss: 0.0796 - accuracy: 0.9719 - val_loss: 1.1228 - val_accuracy: 0.7531\n",
            "Epoch 98/200\n",
            "10/10 [==============================] - 0s 16ms/step - loss: 0.0600 - accuracy: 0.9844 - val_loss: 1.1144 - val_accuracy: 0.7531\n",
            "Epoch 99/200\n",
            "10/10 [==============================] - 0s 15ms/step - loss: 0.0420 - accuracy: 0.9969 - val_loss: 1.1049 - val_accuracy: 0.7531\n",
            "Epoch 100/200\n",
            "10/10 [==============================] - 0s 14ms/step - loss: 0.0847 - accuracy: 0.9719 - val_loss: 1.1085 - val_accuracy: 0.7531\n",
            "Epoch 101/200\n",
            "10/10 [==============================] - 0s 15ms/step - loss: 0.0621 - accuracy: 0.9844 - val_loss: 1.0889 - val_accuracy: 0.7531\n",
            "Epoch 102/200\n",
            "10/10 [==============================] - 0s 16ms/step - loss: 0.0743 - accuracy: 0.9688 - val_loss: 1.0761 - val_accuracy: 0.7531\n",
            "Epoch 103/200\n",
            "10/10 [==============================] - 0s 16ms/step - loss: 0.0463 - accuracy: 0.9875 - val_loss: 1.0990 - val_accuracy: 0.7531\n",
            "Epoch 104/200\n",
            "10/10 [==============================] - 0s 17ms/step - loss: 0.0726 - accuracy: 0.9781 - val_loss: 1.1169 - val_accuracy: 0.7531\n",
            "Epoch 105/200\n",
            "10/10 [==============================] - 0s 11ms/step - loss: 0.0516 - accuracy: 0.9875 - val_loss: 1.1184 - val_accuracy: 0.7531\n",
            "Epoch 106/200\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 0.0567 - accuracy: 0.9875 - val_loss: 1.1288 - val_accuracy: 0.7654\n",
            "Epoch 107/200\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.0667 - accuracy: 0.9781 - val_loss: 1.1034 - val_accuracy: 0.7654\n",
            "Epoch 108/200\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 0.0587 - accuracy: 0.9812 - val_loss: 1.1029 - val_accuracy: 0.7654\n",
            "Epoch 109/200\n",
            "10/10 [==============================] - 0s 11ms/step - loss: 0.0499 - accuracy: 0.9812 - val_loss: 1.1133 - val_accuracy: 0.7654\n",
            "Epoch 110/200\n",
            "10/10 [==============================] - 0s 11ms/step - loss: 0.0534 - accuracy: 0.9875 - val_loss: 1.1664 - val_accuracy: 0.7901\n",
            "Epoch 111/200\n",
            "10/10 [==============================] - 0s 11ms/step - loss: 0.0332 - accuracy: 0.9937 - val_loss: 1.1968 - val_accuracy: 0.7901\n",
            "Epoch 112/200\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.0457 - accuracy: 0.9844 - val_loss: 1.2000 - val_accuracy: 0.7654\n",
            "Epoch 113/200\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 0.0547 - accuracy: 0.9937 - val_loss: 1.2190 - val_accuracy: 0.7901\n",
            "Epoch 114/200\n",
            "10/10 [==============================] - 0s 13ms/step - loss: 0.0470 - accuracy: 0.9906 - val_loss: 1.2276 - val_accuracy: 0.7654\n",
            "Epoch 115/200\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.0633 - accuracy: 0.9750 - val_loss: 1.2334 - val_accuracy: 0.7654\n",
            "Epoch 116/200\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 0.0418 - accuracy: 0.9906 - val_loss: 1.2057 - val_accuracy: 0.7654\n",
            "Epoch 117/200\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 0.0513 - accuracy: 0.9875 - val_loss: 1.1869 - val_accuracy: 0.7778\n",
            "Epoch 118/200\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.0352 - accuracy: 0.9937 - val_loss: 1.2067 - val_accuracy: 0.7654\n",
            "Epoch 119/200\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.0326 - accuracy: 0.9937 - val_loss: 1.2276 - val_accuracy: 0.7654\n",
            "Epoch 120/200\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.0640 - accuracy: 0.9781 - val_loss: 1.2651 - val_accuracy: 0.7531\n",
            "Epoch 121/200\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 0.0327 - accuracy: 0.9969 - val_loss: 1.2856 - val_accuracy: 0.7778\n",
            "Epoch 122/200\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 0.0346 - accuracy: 0.9937 - val_loss: 1.2407 - val_accuracy: 0.7531\n",
            "Epoch 123/200\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.0468 - accuracy: 0.9906 - val_loss: 1.2147 - val_accuracy: 0.7407\n",
            "Epoch 124/200\n",
            "10/10 [==============================] - 0s 12ms/step - loss: 0.0265 - accuracy: 0.9937 - val_loss: 1.2096 - val_accuracy: 0.7531\n",
            "Epoch 125/200\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 0.0367 - accuracy: 0.9969 - val_loss: 1.2085 - val_accuracy: 0.7531\n",
            "Epoch 126/200\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 0.0510 - accuracy: 0.9812 - val_loss: 1.2373 - val_accuracy: 0.7531\n",
            "Epoch 127/200\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.0536 - accuracy: 0.9906 - val_loss: 1.2250 - val_accuracy: 0.7531\n",
            "Epoch 128/200\n",
            "10/10 [==============================] - 0s 11ms/step - loss: 0.0331 - accuracy: 0.9906 - val_loss: 1.2192 - val_accuracy: 0.7531\n",
            "Epoch 129/200\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 0.0535 - accuracy: 0.9812 - val_loss: 1.2508 - val_accuracy: 0.7654\n",
            "Epoch 130/200\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.0583 - accuracy: 0.9844 - val_loss: 1.2160 - val_accuracy: 0.7778\n",
            "Epoch 131/200\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 0.0419 - accuracy: 0.9906 - val_loss: 1.1867 - val_accuracy: 0.8025\n",
            "Epoch 132/200\n",
            "10/10 [==============================] - 0s 11ms/step - loss: 0.0435 - accuracy: 0.9844 - val_loss: 1.1646 - val_accuracy: 0.8025\n",
            "Epoch 133/200\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 0.0303 - accuracy: 0.9937 - val_loss: 1.1674 - val_accuracy: 0.7901\n",
            "Epoch 134/200\n",
            "10/10 [==============================] - 0s 12ms/step - loss: 0.0583 - accuracy: 0.9750 - val_loss: 1.2105 - val_accuracy: 0.7778\n",
            "Epoch 135/200\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 0.0379 - accuracy: 0.9906 - val_loss: 1.2757 - val_accuracy: 0.7778\n",
            "Epoch 136/200\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.0240 - accuracy: 1.0000 - val_loss: 1.2940 - val_accuracy: 0.7778\n",
            "Epoch 137/200\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.0462 - accuracy: 0.9875 - val_loss: 1.2773 - val_accuracy: 0.7778\n",
            "Epoch 138/200\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 0.0541 - accuracy: 0.9906 - val_loss: 1.2627 - val_accuracy: 0.7407\n",
            "Epoch 139/200\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 0.0294 - accuracy: 0.9969 - val_loss: 1.2379 - val_accuracy: 0.7531\n",
            "Epoch 140/200\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 0.0366 - accuracy: 0.9875 - val_loss: 1.2365 - val_accuracy: 0.7654\n",
            "Epoch 141/200\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 0.0401 - accuracy: 0.9875 - val_loss: 1.2770 - val_accuracy: 0.7654\n",
            "Epoch 142/200\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 0.0394 - accuracy: 0.9875 - val_loss: 1.3234 - val_accuracy: 0.7531\n",
            "Epoch 143/200\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 0.0260 - accuracy: 0.9937 - val_loss: 1.3570 - val_accuracy: 0.7531\n",
            "Epoch 144/200\n",
            "10/10 [==============================] - 0s 13ms/step - loss: 0.0227 - accuracy: 0.9937 - val_loss: 1.3515 - val_accuracy: 0.7531\n",
            "Epoch 145/200\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.0281 - accuracy: 0.9937 - val_loss: 1.3373 - val_accuracy: 0.7654\n",
            "Epoch 146/200\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 0.0272 - accuracy: 0.9937 - val_loss: 1.3257 - val_accuracy: 0.7654\n",
            "Epoch 147/200\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.0389 - accuracy: 0.9906 - val_loss: 1.3139 - val_accuracy: 0.7654\n",
            "Epoch 148/200\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.0318 - accuracy: 0.9875 - val_loss: 1.2943 - val_accuracy: 0.7531\n",
            "Epoch 149/200\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.0445 - accuracy: 0.9875 - val_loss: 1.3115 - val_accuracy: 0.7654\n",
            "Epoch 150/200\n",
            "10/10 [==============================] - 0s 11ms/step - loss: 0.0271 - accuracy: 0.9937 - val_loss: 1.3136 - val_accuracy: 0.7901\n",
            "Epoch 151/200\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 0.0296 - accuracy: 0.9906 - val_loss: 1.3010 - val_accuracy: 0.7901\n",
            "Epoch 152/200\n",
            "10/10 [==============================] - 0s 11ms/step - loss: 0.0317 - accuracy: 0.9875 - val_loss: 1.2712 - val_accuracy: 0.7778\n",
            "Epoch 153/200\n",
            "10/10 [==============================] - 0s 12ms/step - loss: 0.0401 - accuracy: 0.9844 - val_loss: 1.2826 - val_accuracy: 0.7778\n",
            "Epoch 154/200\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.0236 - accuracy: 1.0000 - val_loss: 1.3061 - val_accuracy: 0.7778\n",
            "Epoch 155/200\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.0189 - accuracy: 1.0000 - val_loss: 1.3233 - val_accuracy: 0.7531\n",
            "Epoch 156/200\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.0484 - accuracy: 0.9875 - val_loss: 1.3691 - val_accuracy: 0.7531\n",
            "Epoch 157/200\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 0.0326 - accuracy: 0.9937 - val_loss: 1.3692 - val_accuracy: 0.7654\n",
            "Epoch 158/200\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.0401 - accuracy: 0.9906 - val_loss: 1.3505 - val_accuracy: 0.7778\n",
            "Epoch 159/200\n",
            "10/10 [==============================] - 0s 11ms/step - loss: 0.0265 - accuracy: 0.9937 - val_loss: 1.3344 - val_accuracy: 0.7778\n",
            "Epoch 160/200\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.0329 - accuracy: 0.9969 - val_loss: 1.2694 - val_accuracy: 0.7778\n",
            "Epoch 161/200\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.0267 - accuracy: 0.9906 - val_loss: 1.2497 - val_accuracy: 0.7778\n",
            "Epoch 162/200\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.0371 - accuracy: 0.9844 - val_loss: 1.2667 - val_accuracy: 0.7654\n",
            "Epoch 163/200\n",
            "10/10 [==============================] - 0s 12ms/step - loss: 0.0242 - accuracy: 0.9937 - val_loss: 1.2987 - val_accuracy: 0.7654\n",
            "Epoch 164/200\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.0416 - accuracy: 0.9844 - val_loss: 1.2848 - val_accuracy: 0.7531\n",
            "Epoch 165/200\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.0634 - accuracy: 0.9812 - val_loss: 1.2537 - val_accuracy: 0.7654\n",
            "Epoch 166/200\n",
            "10/10 [==============================] - 0s 11ms/step - loss: 0.0267 - accuracy: 0.9906 - val_loss: 1.2895 - val_accuracy: 0.7654\n",
            "Epoch 167/200\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.0418 - accuracy: 0.9875 - val_loss: 1.3321 - val_accuracy: 0.7654\n",
            "Epoch 168/200\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.0332 - accuracy: 0.9906 - val_loss: 1.3475 - val_accuracy: 0.7654\n",
            "Epoch 169/200\n",
            "10/10 [==============================] - 0s 11ms/step - loss: 0.0279 - accuracy: 0.9906 - val_loss: 1.3226 - val_accuracy: 0.7654\n",
            "Epoch 170/200\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 0.0429 - accuracy: 0.9875 - val_loss: 1.3099 - val_accuracy: 0.7778\n",
            "Epoch 171/200\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 0.0383 - accuracy: 0.9844 - val_loss: 1.3253 - val_accuracy: 0.7654\n",
            "Epoch 172/200\n",
            "10/10 [==============================] - 0s 11ms/step - loss: 0.0258 - accuracy: 0.9937 - val_loss: 1.3040 - val_accuracy: 0.7654\n",
            "Epoch 173/200\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.0334 - accuracy: 0.9937 - val_loss: 1.2948 - val_accuracy: 0.7654\n",
            "Epoch 174/200\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.0206 - accuracy: 0.9969 - val_loss: 1.3213 - val_accuracy: 0.7654\n",
            "Epoch 175/200\n",
            "10/10 [==============================] - 0s 12ms/step - loss: 0.0444 - accuracy: 0.9875 - val_loss: 1.3284 - val_accuracy: 0.7531\n",
            "Epoch 176/200\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.0485 - accuracy: 0.9812 - val_loss: 1.3267 - val_accuracy: 0.7531\n",
            "Epoch 177/200\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.0412 - accuracy: 0.9844 - val_loss: 1.3553 - val_accuracy: 0.7531\n",
            "Epoch 178/200\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.0354 - accuracy: 0.9906 - val_loss: 1.3630 - val_accuracy: 0.7778\n",
            "Epoch 179/200\n",
            "10/10 [==============================] - 0s 11ms/step - loss: 0.0189 - accuracy: 1.0000 - val_loss: 1.3507 - val_accuracy: 0.7778\n",
            "Epoch 180/200\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.0202 - accuracy: 0.9969 - val_loss: 1.3348 - val_accuracy: 0.7778\n",
            "Epoch 181/200\n",
            "10/10 [==============================] - 0s 11ms/step - loss: 0.0330 - accuracy: 0.9875 - val_loss: 1.3209 - val_accuracy: 0.7654\n",
            "Epoch 182/200\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 0.0372 - accuracy: 0.9937 - val_loss: 1.3286 - val_accuracy: 0.7778\n",
            "Epoch 183/200\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 0.0212 - accuracy: 0.9969 - val_loss: 1.2992 - val_accuracy: 0.7778\n",
            "Epoch 184/200\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.0231 - accuracy: 0.9937 - val_loss: 1.2379 - val_accuracy: 0.7778\n",
            "Epoch 185/200\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 0.0273 - accuracy: 0.9906 - val_loss: 1.2247 - val_accuracy: 0.7901\n",
            "Epoch 186/200\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 0.0288 - accuracy: 0.9906 - val_loss: 1.2756 - val_accuracy: 0.7901\n",
            "Epoch 187/200\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 0.0368 - accuracy: 0.9875 - val_loss: 1.3019 - val_accuracy: 0.7778\n",
            "Epoch 188/200\n",
            "10/10 [==============================] - 0s 11ms/step - loss: 0.0578 - accuracy: 0.9719 - val_loss: 1.2489 - val_accuracy: 0.7778\n",
            "Epoch 189/200\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 0.0339 - accuracy: 0.9937 - val_loss: 1.2625 - val_accuracy: 0.7901\n",
            "Epoch 190/200\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.0407 - accuracy: 0.9844 - val_loss: 1.2776 - val_accuracy: 0.8025\n",
            "Epoch 191/200\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 0.0236 - accuracy: 0.9969 - val_loss: 1.2720 - val_accuracy: 0.8025\n",
            "Epoch 192/200\n",
            "10/10 [==============================] - 0s 12ms/step - loss: 0.0199 - accuracy: 0.9969 - val_loss: 1.3178 - val_accuracy: 0.7901\n",
            "Epoch 193/200\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.0372 - accuracy: 0.9906 - val_loss: 1.3049 - val_accuracy: 0.7901\n",
            "Epoch 194/200\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.0221 - accuracy: 0.9969 - val_loss: 1.2689 - val_accuracy: 0.7901\n",
            "Epoch 195/200\n",
            "10/10 [==============================] - 0s 10ms/step - loss: 0.0499 - accuracy: 0.9906 - val_loss: 1.2513 - val_accuracy: 0.7901\n",
            "Epoch 196/200\n",
            "10/10 [==============================] - 0s 9ms/step - loss: 0.0335 - accuracy: 0.9937 - val_loss: 1.2587 - val_accuracy: 0.7778\n",
            "Epoch 197/200\n",
            "10/10 [==============================] - 0s 11ms/step - loss: 0.0321 - accuracy: 0.9906 - val_loss: 1.2814 - val_accuracy: 0.7778\n",
            "Epoch 198/200\n",
            "10/10 [==============================] - 0s 11ms/step - loss: 0.0169 - accuracy: 0.9969 - val_loss: 1.2918 - val_accuracy: 0.7778\n",
            "Epoch 199/200\n",
            "10/10 [==============================] - 0s 16ms/step - loss: 0.0295 - accuracy: 0.9906 - val_loss: 1.2529 - val_accuracy: 0.7654\n",
            "Epoch 200/200\n",
            "10/10 [==============================] - 0s 16ms/step - loss: 0.0320 - accuracy: 0.9906 - val_loss: 1.2753 - val_accuracy: 0.7778\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 1.2753 - accuracy: 0.7778\n",
            "Validation Accuracy: 0.7778\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "import json\n",
        "import pickle\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from keras.models import Sequential\n",
        "from keras.layers import GlobalMaxPooling1D, Dense, Activation, Dropout, Embedding,Conv1D\n",
        "import random\n",
        "\n",
        "\n",
        "with open(\"intents.json\") as file:\n",
        "    intents = json.load(file)\n",
        "\n",
        "df = pd.DataFrame(columns=['Pattern', 'Tag'])\n",
        "\n",
        "def extract_json_info(json_file, df):\n",
        "    # Iterate over each intent in the JSON file\n",
        "    for intent in json_file['intents']:\n",
        "        # Iterate over each pattern in the current intent\n",
        "        for pattern in intent['patterns']:\n",
        "            # Create a list containing the pattern and its associated tag\n",
        "            sentence_tag = [pattern, intent['tag']]\n",
        "            # Append the pattern and tag to the DataFrame\n",
        "            df.loc[len(df.index)] = sentence_tag\n",
        "     # Return the updated DataFrame\n",
        "    return df\n",
        "df = extract_json_info(intents, df)\n",
        "labels = df['Tag'].unique().tolist()\n",
        "labels = [s.strip() for s in labels]\n",
        "#count each tag\n",
        "tag_counts = df['Tag'].value_counts()\n",
        "\n",
        "# Tokenize the text patterns\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(df['Pattern'])\n",
        "\n",
        "# Convert text patterns to numerical sequences\n",
        "X = tokenizer.texts_to_sequences(df['Pattern'])\n",
        "\n",
        "# Pad sequences to ensure uniform length\n",
        "max_sequence_length = max(len(seq) for seq in X)\n",
        "X_padded = pad_sequences(X, maxlen=max_sequence_length, padding='post')\n",
        "\n",
        "# Convert tags to numerical labels\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(df['Tag'])\n",
        "\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_padded, y, test_size=0.2, random_state=42)\n",
        "\n",
        "#Model Defination\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=100),\n",
        "    Conv1D(filters=128, kernel_size=5, activation='relu'),\n",
        "    GlobalMaxPooling1D(),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(len(label_encoder.classes_), activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Fit the model to training data\n",
        "history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=200, batch_size=32)\n",
        "\n",
        "\n",
        "\n",
        "# Evaluate the model on the validation set\n",
        "val_loss, val_accuracy = model.evaluate(X_val, y_val)\n",
        "print(f'Validation Accuracy: {val_accuracy:.4f}')\n",
        "\n",
        "# Save the model\n",
        "model.save('mymodel.h5')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"mymodel.h5\")"
      ],
      "metadata": {
        "id": "CSyFXFSxaNQU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import load_model\n",
        "\n",
        "# Loading the saved model\n",
        "loaded_model = load_model('mymodel.h5')\n",
        "\n",
        "# Function to preprocess input sentence\n",
        "def preprocess_input_sentence(sentence, tokenizer, max_sequence_length):\n",
        "    # Tokenize input sentence\n",
        "    input_sequence = tokenizer.texts_to_sequences([sentence])\n",
        "    # Pad sequences\n",
        "    padded_sequence = pad_sequences(input_sequence, maxlen=max_sequence_length, padding='post')\n",
        "    return padded_sequence\n",
        "\n",
        "# Function to get response\n",
        "def get_response(sentence, loaded_model, tokenizer, max_sequence_length, label_encoder):\n",
        "    # Preprocess input sentence\n",
        "    preprocessed_input = preprocess_input_sentence(sentence, tokenizer, max_sequence_length)\n",
        "    # Predict label\n",
        "    predicted_label = loaded_model.predict(preprocessed_input).argmax(axis=-1)\n",
        "    # Convert label to tag\n",
        "    predicted_tag = label_encoder.inverse_transform(predicted_label)\n",
        "    return predicted_tag\n",
        "\n",
        "#example\n",
        "sentence = \"who are u\"\n",
        "# Process the input sentence and get response\n",
        "response = get_response(sentence, loaded_model, tokenizer, max_sequence_length, label_encoder)\n",
        "\n",
        "# Print the response\n",
        "print(\"Response:\", response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lIX8Fr0bUMb4",
        "outputId": "a3b692de-356f-4ad7-e3c6-0e820da59b6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 81ms/step\n",
            "Response: ['name']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the saved model\n",
        "loaded_model = load_model('mymodel.h5')\n",
        "\n",
        "# Function to preprocess input sentence\n",
        "def preprocess_input_sentence(sentence, tokenizer, max_sequence_length):\n",
        "    # Tokenize input sentence\n",
        "    input_sequence = tokenizer.texts_to_sequences([sentence])\n",
        "    # Pad sequences\n",
        "    padded_sequence = pad_sequences(input_sequence, maxlen=max_sequence_length, padding='post')\n",
        "    return padded_sequence\n",
        "\n",
        "# Function to generate response\n",
        "def generate_response(sentence, loaded_model, tokenizer, max_sequence_length, label_encoder):\n",
        "    # Preprocess input sentence\n",
        "    preprocessed_input = preprocess_input_sentence(sentence, tokenizer, max_sequence_length)\n",
        "    # Predict probabilities for all classes\n",
        "    predicted_probabilities = loaded_model.predict(preprocessed_input)\n",
        "    # Get the index of the class with the highest probability\n",
        "    predicted_label_index = np.argmax(predicted_probabilities, axis=-1)\n",
        "    # Get the predicted tag based on the index\n",
        "    predicted_tag = label_encoder.inverse_transform(predicted_label_index)\n",
        "    return predicted_tag\n",
        "\n",
        "# Example of conversation loop\n",
        "print(\"Chatbot: Hello! How can I assist you today? (Type 'exit' to end)\")\n",
        "while True:\n",
        "    user_input = input(\"You: \")\n",
        "    if user_input.lower() == 'exit':\n",
        "        print(\"Chatbot: Goodbye! Have a great day!\")\n",
        "        break\n",
        "    # Get response from the chatbot\n",
        "    response = generate_response(user_input, loaded_model, tokenizer, max_sequence_length, label_encoder)\n",
        "    print(\"Chatbot:\", response[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pcYa4bGsVfxX",
        "outputId": "28568e8f-6f3a-4475-ba6b-ae934674de6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chatbot: Hello! How can I assist you today? (Type 'exit' to end)\n",
            "You: go\n",
            "1/1 [==============================] - 0s 80ms/step\n",
            "Chatbot: goodbye\n",
            "You: hello\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "Chatbot: greeting\n",
            "You: exit\n",
            "Chatbot: Goodbye! Have a great day!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the model\n",
        "loaded_model = load_model('mymodel.h5')\n",
        "\n",
        "# Function to preprocess input sentence\n",
        "def preprocess_input_sentence(sentence, tokenizer, max_sequence_length):\n",
        "    # Tokenize input sentence\n",
        "    input_sequence = tokenizer.texts_to_sequences([sentence])\n",
        "    # Pad sequences\n",
        "    padded_sequence = pad_sequences(input_sequence, maxlen=max_sequence_length, padding='post')\n",
        "    return padded_sequence\n",
        "\n",
        "# Function to get response\n",
        "def get_response(sentence, loaded_model, tokenizer, max_sequence_length, intents, df):\n",
        "    # Preprocess input sentence\n",
        "    preprocessed_input = preprocess_input_sentence(sentence, tokenizer, max_sequence_length)\n",
        "    # Predict label\n",
        "    predicted_label = loaded_model.predict(preprocessed_input).argmax(axis=-1)\n",
        "    # Convert label to tag\n",
        "    predicted_tag = label_encoder.inverse_transform(predicted_label)\n",
        "    # Get the response based on the predicted tag\n",
        "    if predicted_tag[0] in df['Tag'].values:\n",
        "        response_row = df[df['Tag'] == predicted_tag[0]].iloc[0]\n",
        "        response = response_row['Tag'] + \": \" + df['Pattern']\n",
        "    else:\n",
        "        response = intents.get(predicted_tag[0], \"I'm sorry, I didn't understand that.\")\n",
        "    return response\n",
        "\n",
        "# Example usage\n",
        "sentence = \"Hi\"\n",
        "response = get_response(sentence, loaded_model, tokenizer, max_sequence_length, intents, df)\n",
        "print(\"Chatbot:\", response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gq3I1c0SVqa3",
        "outputId": "22066257-8d5f-4cea-9a87-cf61e444ed45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 78ms/step\n",
            "Chatbot: 0                     canteen: Hi\n",
            "1           canteen: How are you?\n",
            "2       canteen: Is anyone there?\n",
            "3                  canteen: Hello\n",
            "4               canteen: Good day\n",
            "                  ...            \n",
            "400      canteen: ragging history\n",
            "401    canteen: ragging incidents\n",
            "402                  canteen: hod\n",
            "403             canteen: hod name\n",
            "404       canteen: who is the hod\n",
            "Name: Pattern, Length: 405, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to preprocess input sentence\n",
        "def preprocess_input_sentence(sentence, tokenizer, max_sequence_length):\n",
        "    # Tokenize input sentence\n",
        "    input_sequence = tokenizer.texts_to_sequences([sentence])\n",
        "    # Pad sequences\n",
        "    padded_sequence = pad_sequences(input_sequence, maxlen=max_sequence_length, padding='post')\n",
        "    return padded_sequence\n",
        "\n",
        "# Function to get response\n",
        "def get_response(sentence, loaded_model, tokenizer, max_sequence_length, intents):\n",
        "    # Preprocess input sentence\n",
        "    preprocessed_input = preprocess_input_sentence(sentence, tokenizer, max_sequence_length)\n",
        "    # Predict label\n",
        "    predicted_label = loaded_model.predict(preprocessed_input).argmax(axis=-1)\n",
        "    # Convert label to tag\n",
        "    predicted_tag = label_encoder.inverse_transform(predicted_label)\n",
        "\n",
        "    # Iterate through the intents to find the matching tag\n",
        "    for intent in intents[\"intents\"]:\n",
        "        if intent[\"tag\"] == predicted_tag[0]:\n",
        "            # Select a random response from the list of responses\n",
        "            response = random.choice(intent[\"responses\"])\n",
        "            return response\n",
        "\n",
        "    # If no matching intent is found, return a default message\n",
        "    return \"I'm sorry, I didn't understand that.\"\n",
        "\n",
        "\n",
        "print(\"Chatbot: Hello! How can I assist you today? (Type 'exit' to end)\")\n",
        "while True:\n",
        "    # Get user input\n",
        "    sentence = input(\"You: \")\n",
        "    # Check if the user wants to exit\n",
        "    if sentence.lower() == 'exit':\n",
        "        print(\"Chatbot: Goodbye! Have a great day!\")\n",
        "        break\n",
        "    # Get response from chatbot\n",
        "    response = get_response(sentence, loaded_model, tokenizer, max_sequence_length, intents)\n",
        "    print(\"Chatbot:\", response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ED1WistkV0F2",
        "outputId": "7b0f549a-87a1-4522-a816-c896860a30f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chatbot: Hello! How can I assist you today? (Type 'exit' to end)\n",
            "You: hi\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Chatbot: Our university has canteen with variety of food available\n",
            "You: canteen\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Chatbot: Our university has canteen with variety of food available\n",
            "You: fines\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Chatbot: Our university's Engineering department provides fully AC Lab with internet connection, smart classroom, Auditorium, library,canteen\n",
            "You: heyy\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "Chatbot: Good to see you again!\n",
            "You: Good evening\n",
            "1/1 [==============================] - 0s 104ms/step\n",
            "Chatbot: welcome, anything else i can assist you with?\n",
            "You: can you tell me your name?\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "Chatbot: I am your helper\n",
            "You: what are the working hours?\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "Chatbot: College is open 8am-5pm Monday-Saturday!\n",
            "You: Do you have contact info\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Chatbot: You can contact at: NUMBER\n",
            "You: what all courses are offered?\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "Chatbot: Our university offers Information Technology, computer Engineering, Mechanical engineering,Chemical engineering, Civil engineering and extc Engineering.\n",
            "You: fee detail?\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Chatbot: For Fee detail visit <a target=\"_blank\" href=\"LINK\"> here</a>\n",
            "You: uniform?\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "Chatbot: ENTER YOUR OWN UNIVERSITY UNIFORM CIRCULER\n",
            "You: how many commities\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "Chatbot: There is one huge and spacious library.timings are 8am to 6pm and for more visit <a target=\"blank\" href=\"ADD LIBRARY DETAIL LINK\">here</a>\n",
            "You: bye\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "Chatbot: Sad to see you go :(\n",
            "You: exit\n",
            "Chatbot: Goodbye! Have a great day!\n"
          ]
        }
      ]
    }
  ]
}